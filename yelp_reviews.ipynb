{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_review.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nteract": {
      "version": "0.11.2"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUVgL1wNNajZ",
        "outputId": "fe103965-7304-4648-e797-5527adfb8761"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.2.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 0 B/88.7 kB 0%] [Connecting to cloud.r-pr\r                                                                               \rGet:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connecting to cloud\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 88.7 kB/88.7 kB 100%] [2 InRelease 15.9 k\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r                                                                               \rHit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r                                                                               \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [73.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [699 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,218 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,867 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,426 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [665 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,439 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,812 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [928 kB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [808 kB]\n",
            "Fetched 14.3 MB in 3s (4,805 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uinB2WIU8Cwy"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"NaiveBayes\").getOrCreate()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnI0zdY5NYCJ",
        "outputId": "8a916c3c-9aa3-478e-ef88-b0723f456360"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show(truncate=False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------------------------------------------------------------------------------------------------------+\n",
            "|class   |text                                                                                                           |\n",
            "+--------+---------------------------------------------------------------------------------------------------------------+\n",
            "|positive|Wow... Loved this place.                                                                                       |\n",
            "|negative|Crust is not good.                                                                                             |\n",
            "|negative|Not tasty and the texture was just nasty.                                                                      |\n",
            "|positive|Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.                        |\n",
            "|positive|The selection on the menu was great and so were the prices.                                                    |\n",
            "|negative|Now I am getting angry and I want my damn pho.                                                                 |\n",
            "|negative|Honeslty it didn't taste THAT fresh.)                                                                          |\n",
            "|negative|The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.|\n",
            "|positive|The fries were great too.                                                                                      |\n",
            "|positive|A great touch.                                                                                                 |\n",
            "|positive|Service was very prompt.                                                                                       |\n",
            "|negative|Would not go back.                                                                                             |\n",
            "|negative|The cashier had no care what so ever on what I had to say it still ended up being wayyy overpriced.            |\n",
            "|positive|I tried the Cape Cod ravoli, chicken,with cranberry...mmmm!                                                    |\n",
            "|negative|I was disgusted because I was pretty sure that was human hair.                                                 |\n",
            "|negative|I was shocked because no signs indicate cash only.                                                             |\n",
            "|positive|Highly recommended.                                                                                            |\n",
            "|negative|Waitress was a little slow in service.                                                                         |\n",
            "|negative|This place is not worth your time, let alone Vegas.                                                            |\n",
            "|negative|did not like at all.                                                                                           |\n",
            "+--------+---------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BbzYExyNYCR",
        "outputId": "8e61f8d9-8c90-435c-85d8-49c8bbb1990d"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature \n",
        "df2 = df.withColumn('length', length(df['text']))\n",
        "df2.show()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od7Qj0sxNYCW"
      },
      "source": [
        "### Feature Transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59dwxefsNYCX"
      },
      "source": [
        "# Create all the features to the data set\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "pos_neg_to_num = StringIndexer(inputCol = 'class', outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "stopremover = StopWordsRemover(inputCol = 'token_text', outputCol = \"stop_tokens\")\n",
        "HashingTF=HashingTF(inputCol = 'stop_tokens', outputCol='hash_tokens')\n",
        "idf=IDF(inputCol='hash_tokens', outputCol='idf_tokens')\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssO0_Q5NYCb"
      },
      "source": [
        "# Create feature vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "\n",
        "clean_up = VectorAssembler(inputCols=['idf_tokens', 'length'], outputCol='features')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_YyUpR3NYCf"
      },
      "source": [
        "# Create a and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremover, HashingTF, idf, clean_up])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBViHQOaNYCj"
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(df2)\n",
        "cleaned = cleaner.transform(df2)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDODyxF7NYCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b26cd19-fa3a-4e82-8978-542aace7689b"
      },
      "source": [
        "# Show label and resulting features\n",
        "cleaned.select(['label', 'features']).show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  1.0|(262145,[177414,2...|\n",
            "|  0.0|(262145,[49815,23...|\n",
            "|  0.0|(262145,[109649,1...|\n",
            "|  1.0|(262145,[53101,68...|\n",
            "|  1.0|(262145,[15370,77...|\n",
            "|  0.0|(262145,[98142,13...|\n",
            "|  0.0|(262145,[59172,22...|\n",
            "|  0.0|(262145,[63420,85...|\n",
            "|  1.0|(262145,[53777,17...|\n",
            "|  1.0|(262145,[221827,2...|\n",
            "|  1.0|(262145,[43756,22...|\n",
            "|  0.0|(262145,[127310,1...|\n",
            "|  0.0|(262145,[407,3153...|\n",
            "|  1.0|(262145,[18098,93...|\n",
            "|  0.0|(262145,[23071,12...|\n",
            "|  0.0|(262145,[129941,1...|\n",
            "|  1.0|(262145,[19633,21...|\n",
            "|  0.0|(262145,[27707,65...|\n",
            "|  0.0|(262145,[20891,27...|\n",
            "|  0.0|(262145,[8287,208...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzfCQmrVNYCr"
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeckHhg5NYCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93d8d7c-26ef-48b1-e5b4-d23b09c71e3a"
      },
      "source": [
        "# Tranform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|         hash_tokens|          idf_tokens|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative|\"The burger... I ...|    86|  0.0|[\"the, burger...,...|[\"the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-807.26625494048...|[0.99999999999344...|       0.0|\n",
            "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-83.140966375631...|[2.64191318672794...|       1.0|\n",
            "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-83.140966375631...|[2.64191318672794...|       1.0|\n",
            "|negative|After the disappo...|    61|  0.0|[after, the, disa...|[disappointing, d...|(262144,[55655,14...|(262144,[55655,14...|(262145,[55655,14...|[-469.86141538250...|[0.99999983348365...|       0.0|\n",
            "|negative|After waiting an ...|    75|  0.0|[after, waiting, ...|[waiting, hour, s...|(262144,[54530,67...|(262144,[54530,67...|(262145,[54530,67...|[-550.21528920618...|[0.99999996928237...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVFrWcHINYCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3769de2-de9a-4e9e-fbc5-3427fdf06beb"
      },
      "source": [
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model at predicting reviews was: 0.746542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpKc638NlCQ"
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    }
  ]
}